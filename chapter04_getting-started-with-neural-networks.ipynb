{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
    "\n",
    "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
    "\n",
    "This notebook was generated for TensorFlow 2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Getting started with neural networks: classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Classifying movie reviews: a binary classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loading the IMDB dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\davidd\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\davidd\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0] # 0 stands for negative and 1 stands for positive\n",
    "# the train_data is a list of word indices (encoding a sequence of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([min(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Decoding reviews back to text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = \" \".join(\n",
    "    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Encoding the integer sequences via multi-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Building your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(14, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intermediate layers will use relu as their activation function, and the final layer will use a sigmoid activation so as to output a probability (a score between 0 and 1, indicating how likely the sample is to have the target \"1\": how likely the review is to be positive. A relu (rectified linear unit) is a function meant to zero out negative values, whereas a sigmoid \"squashes\" arbitrary values into the ```[0,1]``` interval, outputting something that can be interpreted as a probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. relu is the most popular activation function in deep learning, but there are many other candidates, which all come with similarly strange names: prelu, elu, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Compiling the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you need to choose a loss function and an optimizer. Because you're facing a binary classification problem and the output of your network is a probablity (you end your network with a single-unit layer with a sigmoid activation), it's best to use the **binary_crossentropy** loss. It isn't the only viable choice: you could use, for instance, **mean_squared_error**. But crossentropy is usually the best choice when you're dealing wtih models that output probablities. _Crossentropy_ is a quantity from the field of Information Theory that measures the distnace between probability distributions orm in this case, between the ground-truth distibution and your predictions. \n",
    "\n",
    "Here's the step where you configure the model with the **rmsprop** optimizer and the **binary_crossentropy** loss function. Note that you'll also monitor accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're passing your optimizer, loss function, and metrics as strings, which is possible because **rmsprop**, **binary_crossentropy**, and **accuracy** are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer or **pass a custom loss function or metric function. The former can be done by passing an optimizer class instance as the optimizer argument, as shown in listing 3.5; the latter can be done by passing function objects as the loss and/or metrics arguments, as show in listing 3.5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Validating your approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 10000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Setting aside a validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training your model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.0918 - accuracy: 0.9743 - val_loss: 0.8051 - val_accuracy: 0.8785\n",
      "Epoch 2/10\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.1185 - accuracy: 0.9714 - val_loss: 0.7230 - val_accuracy: 0.8822\n",
      "Epoch 3/10\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.1197 - accuracy: 0.9684 - val_loss: 0.6216 - val_accuracy: 0.8779\n",
      "Epoch 4/10\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.1277 - accuracy: 0.9679 - val_loss: 0.7300 - val_accuracy: 0.8786\n",
      "Epoch 5/10\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.1321 - accuracy: 0.9680 - val_loss: 0.7658 - val_accuracy: 0.8783\n",
      "Epoch 6/10\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.1446 - accuracy: 0.9674 - val_loss: 0.8026 - val_accuracy: 0.8763\n",
      "Epoch 7/10\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.1393 - accuracy: 0.9677 - val_loss: 0.8825 - val_accuracy: 0.8742\n",
      "Epoch 8/10\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.1374 - accuracy: 0.9699 - val_loss: 1.0408 - val_accuracy: 0.8758\n",
      "Epoch 9/10\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.1311 - accuracy: 0.9715 - val_loss: 1.1322 - val_accuracy: 0.8717\n",
      "Epoch 10/10\n",
      "7500/7500 [==============================] - 12s 2ms/step - loss: 0.1392 - accuracy: 0.9724 - val_loss: 1.2710 - val_accuracy: 0.8749\n",
      "0:02:00.385899\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=2,\n",
    "                    validation_data=(x_val, y_val))\n",
    "end = datetime.now()\n",
    "print(end -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting the training and validation loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting the training and validation accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAApCElEQVR4nO3deZhU5Zn38e+PBoQGV0BUGmjMqIiy2qKCC4kmY6LRuL1KSBSNEjXRSGYSjVk0i+/lZJzEONmGJGoSyRBjIq9xNItEQ1xGbQQNKCoqQqsYRAUU2e/3j3OKri5OdxfQ1VV0/z7Xda46e911qvvc9TzPOc9RRGBmZlaoS7kDMDOzyuQEYWZmmZwgzMwskxOEmZllcoIwM7NMThBmZpbJCcKKJuleSee19brlJGmxpBNKsN+Q9E/p+I8lfbWYdbfjfSZJ+tP2xmnWEvk+iI5N0jt5k9XAOmBTOv3piJje/lFVDkmLgQsj4r423m8AB0TEorZaV1It8BLQLSI2tkmgZi3oWu4ArLQionduvKWToaSuPulYpfDfY2VwFVMnJWmCpAZJV0paBtwiaU9Jd0taLumtdLwmb5sHJF2Yjk+W9KCkG9J1X5L04e1cd4ik2ZJWS7pP0g8k3dZM3MXE+E1JD6X7+5OkvnnLPynpZUkrJH25heNzpKRlkqry5p0m6al0fKykRyS9Lek1Sd+X1L2Zfd0q6Vt5019It3lV0gUF654kaa6kVZKWSro2b/Hs9PVtSe9IOip3bPO2HyfpcUkr09dxxR6bbTzOe0m6Jf0Mb0mambfsVEnz0s/wgqQT0/lNqvMkXZv7niXVplVtn5K0BPhLOv836fewMv0bOSRv+56S/iP9Plemf2M9Jf2PpMsKPs9Tkj6W9VmteU4Qnds+wF7AYGAKyd/DLen0IOA94PstbH8E8CzQF/g28DNJ2o51fwU8BvQBrgU+2cJ7FhPjx4Hzgb2B7sC/AkgaBvwo3f9+6fvVkCEi/hd4F/hAwX5/lY5vAqamn+co4Hjg0hbiJo3hxDSeDwIHAIXtH+8C5wJ7ACcBl+Sd2I5NX/eIiN4R8UjBvvcC/ge4Kf1s3wH+R1Kfgs+w1bHJ0Npx/iVJleUh6b6+m8YwFvgF8IX0MxwLLG7mPbIcBxwM/HM6fS/JcdobeALIrxK9ATgMGEfyd/xFYDPwc+ATuZUkjQQGAPdsQxwGEBEeOslA8o96Qjo+AVgP9Ghh/VHAW3nTD5BUUQFMBhblLasGAthnW9YlOflsBKrzlt8G3FbkZ8qK8St505cCf0jHvwbMyFvWKz0GJzSz728BN6fju5KcvAc3s+4VwJ150wH8Uzp+K/CtdPxm4Pq89Q7MXzdjvzcC303Ha9N1u+Ytnww8mI5/EnisYPtHgMmtHZttOc7AviQn4j0z1vuvXLwt/f2l09fmvue8z7Z/CzHska6zO0kCew8YmbHeLsCbJO06kCSSH5bif6qjDy5BdG7LI2JtbkJStaT/Sovsq0iqNPbIr2YpsCw3EhFr0tHe27jufsCbefMAljYXcJExLssbX5MX0375+46Id4EVzb0XSWnhdEm7AKcDT0TEy2kcB6bVLsvSOP4vSWmiNU1iAF4u+HxHSLo/rdpZCVxc5H5z+365YN7LJL+ec5o7Nk20cpwHknxnb2VsOhB4och4s2w5NpKqJF2fVlOtorEk0jcdemS9V0SsA24HPiGpCzCRpMRj28gJonMrvITtX4CDgCMiYjcaqzSaqzZqC68Be0mqzps3sIX1dyTG1/L3nb5nn+ZWjoinSU6wH6Zp9RIkVVULSX6l7gZcvT0xkJSg8v0KuAsYGBG7Az/O229rlxy+SlIllG8Q8EoRcRVq6TgvJfnO9sjYbinwvmb2+S5J6TFnn4x18j/jx4FTSarhdicpZeRieANY28J7/RyYRFL1tyYKquOsOE4Qlm9XkmL722l99jWlfsP0F3k9cK2k7pKOAj5aohjvAE6WdHTaoPwNWv8f+BVwOckJ8jcFcawC3pE0FLikyBhuByZLGpYmqML4dyX5db42rc//eN6y5SRVO/s3s+97gAMlfVxSV0lnA8OAu4uMrTCOzOMcEa+RtA38MG3M7iYpl0B+Bpwv6XhJXSQNSI8PwDzgnHT9OuDMImJYR1LKqyYppeVi2ExSXfcdSfulpY2j0tIeaULYDPwHLj1sNycIy3cj0JPk19n/An9op/edRNLQu4Kk3v/XJCeGLDeynTFGxALgMyQn/deAt4CGVjb7b5L2mr9ExBt58/+V5OS9GvhJGnMxMdybfoa/AIvS13yXAt+QtJqkzeT2vG3XANcBDym5eurIgn2vAE4m+fW/gqTR9uSCuIt1Iy0f508CG0hKUf8gaYMhIh4jaQT/LrAS+CuNpZqvkvzifwv4Ok1LZFl+QVKCewV4Oo0j378CfwceJ2lz+DeantN+AQwnadOy7eAb5aziSPo1sDAiSl6CsY5L0rnAlIg4utyx7KxcgrCyk3S4pPelVRInktQ7zyxzWLYTS6vvLgWmlTuWnZkThFWCfUguwXyH5Br+SyJiblkjsp2WpH8maa95ndarsawFrmIyM7NMLkGYmVmmDtVZX9++faO2trbcYZiZ7TTmzJnzRkT0y1rWoRJEbW0t9fX15Q7DzGynIanw7vstXMVkZmaZnCDMzCyTE4SZmWVygjAzs0xOEGZmlqnTJ4jp06G2Frp0SV6nT29tCzOzzqFDXea6raZPhylTYE36qJqXX06mASZNKl9cZmaVoFOXIL785cbkkLNmTTLfzKyz69QJYsmSbZtvZtaZdOoEMajwYY+tzDczqySlbkPt1AniuuugurrpvOrqZL6ZWSXLtaG+/DJENLahtmWS6NQJYtIkmDYNBg8GKXmdNq08DdS+msps51EJ/6/t0YbaoZ4HUVdXFztjZ32FV1NBUpIpV7Iys+ZVyv9rly5JyaGQBJs3F78fSXMioi5zmRNE+dXWJsXDQoMHw+LF7R2NmbWkUv5f2yqOlhJEp65iqhS+msqseOWu3qmU/9f2aEN1gqgAvprKrDjt0TDbmkr5f22PNlQniArgq6nMilMJN7dW0v/rpElJddLmzclrW7eBOEFUgEq5mqrcRfdK4+NReSqheqdS/l/bRUR0mOGwww4L2z633RZRXR2RFNyTobo6mV+OWAYPjpCS13LFUCnHo1JUwvcyeHDT7yQ3DB7c/rF0FEB9NHNOLftJvS0HJ4jtVyn/eJVyYq6U4xFRGSfmSvleKiWOjqSlBOHLXA1ou2uqd1SlXEJYKcejUq65r5TvBZJj8uUvJ9VKgwYldf8dsnqnnfg+CGtVpZwAKuXEXCnHo1LiqJTvxdqe74OwVlXKlRmVcglhpRyPSmiUhcr5Xqx9OUEYUDlXZlTKiblSjkelnJgr5XuxdtZc48TOOLiRumOohEbZSlFJjbL+Xjom3EhttvNyo6yVUkttEJ36mdRmO4NJk5wQrDzcBmFmZpmcIMzMLJMThJmZZXKCMDOzTE4QZmaWyQnCzMwylTRBSDpR0rOSFkm6KmP5npLulPSUpMckHZq3bA9Jd0haKOkZSUeVMlYzM2uqZAlCUhXwA+DDwDBgoqRhBatdDcyLiBHAucD38pZ9D/hDRAwFRgLPlCpWMzPbWilLEGOBRRHxYkSsB2YApxasMwyYBRARC4FaSf0l7QYcC/wsXbY+It4uYaxmZlaglAliALA0b7ohnZfvSeB0AEljgcFADbA/sBy4RdJcST+V1CvrTSRNkVQvqX758uVt/RnMzDqtUiYIZcwr7PjpemBPSfOAy4C5wEaSLkDGAD+KiNHAu8BWbRgAETEtIuoioq5fv35tFbuZWadXyr6YGoCBedM1wKv5K0TEKuB8AEkCXkqHaqAhIh5NV72DZhKEmZmVRilLEI8DB0gaIqk7cA5wV/4K6ZVK3dPJC4HZEbEqIpYBSyUdlC47Hni6hLGamVmBkpUgImKjpM8CfwSqgJsjYoGki9PlPwYOBn4haRNJAvhU3i4uA6anCeRF0pKGmZm1Dz8PwsysE/Mzqc3MbJs5QZiZWSYnCDMzy+QEYWZmmZwgzMwskxOEmZllcoIwM7NMThBmZpbJCcLMzDI5QZiZWSYnCDMzy+QEYWZmmZwgzMwskxOEmZllcoIwM7NMThBmZpbJCcLMzDI5QZiZWSYnCDMzy+QEYWZmmZwgzMwskxOEmZllcoIwM7NMThBmZpbJCcLMzDI5QZiZWSYnCDMzy+QEYWZmmZwgzMwsU0kThKQTJT0raZGkqzKW7ynpTklPSXpM0qEFy6skzZV0dynjNDOzrZUsQUiqAn4AfBgYBkyUNKxgtauBeRExAjgX+F7B8s8Bz5QqRjMza14pSxBjgUUR8WJErAdmAKcWrDMMmAUQEQuBWkn9ASTVACcBPy1hjGZm1oxSJogBwNK86YZ0Xr4ngdMBJI0FBgM16bIbgS8Cm1t6E0lTJNVLql++fHkbhG1mZlDaBKGMeVEwfT2wp6R5wGXAXGCjpJOBf0TEnNbeJCKmRURdRNT169dvR2M2M7NU1xLuuwEYmDddA7yav0JErALOB5Ak4KV0OAc4RdJHgB7AbpJui4hPlDBeMzPLU8oSxOPAAZKGSOpOctK/K38FSXukywAuBGZHxKqI+FJE1EREbbrdX5wczMzaV8lKEBGxUdJngT8CVcDNEbFA0sXp8h8DBwO/kLQJeBr4VKniMTOzbaOIwmaBnVddXV3U19eXOwwzs52GpDkRUZe1zHdSm5lZJicIMzPL5ARhZmaZnCDMzCyTE4SZmWVygjAzs0xOEGZmlqnVBCHpZElOJGZmnUwxJ/5zgOclfVvSwaUOyMzMKkOrCSLtA2k08AJwi6RH0i62dy15dGZmVjZFVR2lva7+luShP/sCpwFPSLqshLGZmVkZtdpZn6SPAhcA7wN+CYyNiH9IqiZ5HOh/ljZEM9sZbNiwgYaGBtauXVvuUCxDjx49qKmpoVu3bkVvU0xvrmcB342I2fkzI2KNpAu2MUYz66AaGhrYddddqa2tJXm8i1WKiGDFihU0NDQwZMiQorcrporpGuCx3ISknpJq0zedta2BmlnHtHbtWvr06ePkUIEk0adPn20u3RWTIH5D0+dCb0rnmZk14eRQubbnuykmQXSNiPW5iXS8ewvrm5m1qxUrVjBq1ChGjRrFPvvsw4ABA7ZMr1+/vsVt6+vrufzyy1t9j3HjxrVVuDuNYhLEckmn5CYknQq8UbqQzKwzmD4damuhS5fkdfr07d9Xnz59mDdvHvPmzePiiy9m6tSpW6a7d+/Oxo0bm922rq6Om266qdX3ePjhh7c/wJ1UMQniYuBqSUskLQWuBD5d2rDMrCObPh2mTIGXX4aI5HXKlB1LEoUmT57M5z//ed7//vdz5ZVX8thjjzFu3DhGjx7NuHHjePbZZwF44IEHOPnkkwG49tprueCCC5gwYQL7779/k8TRu3fvLetPmDCBM888k6FDhzJp0iRyT+a85557GDp0KEcffTSXX375lv3mW7x4MccccwxjxoxhzJgxTRLPt7/9bYYPH87IkSO56qqrAFi0aBEnnHACI0eOZMyYMbzwwgttd5Ba0epVTBHxAnCkpN4kjyhdXfqwzKwj+/KXYc2apvPWrEnmT5rUdu/z3HPPcd9991FVVcWqVauYPXs2Xbt25b777uPqq6/mt7/97VbbLFy4kPvvv5/Vq1dz0EEHcckll2x1aejcuXNZsGAB++23H+PHj+ehhx6irq6OT3/608yePZshQ4YwceLEzJj23ntv/vznP9OjRw+ef/55Jk6cSH19Pffeey8zZ87k0Ucfpbq6mjfffBOASZMmcdVVV3Haaaexdu1aNm/enLnfUijmMlcknQQcAvTINXRExDdKGJeZdWBLlmzb/O111llnUVVVBcDKlSs577zzeP7555HEhg0bMrc56aST2GWXXdhll13Ye++9ef3116mpqWmyztixY7fMGzVqFIsXL6Z3797sv//+Wy4jnThxItOmTdtq/xs2bOCzn/0s8+bNo6qqiueeew6A++67j/PPP5/q6moA9tprL1avXs0rr7zCaaedBiT3MrSnYjrr+zFwNnAZIJL7IgaXOC4z68AGDdq2+durV69eW8a/+tWv8v73v5/58+fz+9//vtlLPnfZZZct41VVVZntF1nr5KqZWvPd736X/v378+STT1JfX7+lET0itrrSqNh9lkoxbRDjIuJc4K2I+DpwFDCwtGGZWUd23XWQ/lDeoro6mV8qK1euZMCAAQDceuutbb7/oUOH8uKLL7J48WIAfv3rXzcbx7777kuXLl345S9/yaZNmwD40Ic+xM0338yatO7tzTffZLfddqOmpoaZM2cCsG7dui3L20MxCSKXZtdI2g/YABR/K56ZWYFJk2DaNBg8GKTkddq0tm1/KPTFL36RL33pS4wfP37LSbkt9ezZkx/+8IeceOKJHH300fTv35/dd999q/UuvfRSfv7zn3PkkUfy3HPPbSnlnHjiiZxyyinU1dUxatQobrjhBgB++ctfctNNNzFixAjGjRvHsmXL2jz25qi1Ioykr5L0t3Q88AMggJ9ExNdKH962qauri/r6+nKHYdYpPfPMMxx8cOd+IsA777xD7969iQg+85nPcMABBzB16tRyh7VF1nckaU5E1GWt32IJIn1Q0KyIeDsifkvS9jC0EpODmVm5/eQnP2HUqFEccsghrFy5kk9/eue+I6DFq5giYrOk/yBpdyAi1gHr2iMwM7OdzdSpUyuqxLCjimmD+JOkM+ROVszMOpVi7oP4PNAL2ChpLcmlrhERu5U0MjMzK6ti7qT2o0XNzDqhYp4od2zW/MIHCJmZWcdSTBvEF/KGrwK/B64tZueSTpT0rKRFkq7KWL6npDslPSXpMUmHpvMHSrpf0jOSFkj6XNGfyMw6nQkTJvDHP/6xybwbb7yRSy+9tMVtcpfFf+QjH+Htt9/eap1rr712y/0IzZk5cyZPP/30lumvfe1r3HfffdsQfeVqNUFExEfzhg8ChwKvt7adpCqS+yY+DAwDJkoaVrDa1cC8iBgBnAt8L52/EfiXiDgYOBL4TMa2ZmZA0u/RjBkzmsybMWNGsx3mFbrnnnvYY489tuu9CxPEN77xDU444YTt2lelKaYEUaiBJEm0ZiywKCJeTB8yNAM4tWCdYcAsgIhYCNRK6h8Rr0XEE+n81cAzwIDtiNXMOoEzzzyTu+++m3XrkqvwFy9ezKuvvsrRRx/NJZdcQl1dHYcccgjXXHNN5va1tbW88UbymJvrrruOgw46iBNOOGFLl+CQ3ONw+OGHM3LkSM444wzWrFnDww8/zF133cUXvvAFRo0axQsvvMDkyZO54447AJg1axajR49m+PDhXHDBBVviq62t5ZprrmHMmDEMHz6chQsXbhVTJXQLXkwbxH+S3D0NSUIZBTxZxL4HAEvzphuAIwrWeRI4HXhQ0liSG/FqyCuhpM+/Hg082kx8U4ApAIPauqcvM9suV1wB8+a17T5HjYIbb8xe1qdPH8aOHcsf/vAHTj31VGbMmMHZZ5+NJK677jr22msvNm3axPHHH89TTz3FiBEjMvczZ84cZsyYwdy5c9m4cSNjxozhsMMOA+D000/noosuAuArX/kKP/vZz7jssss45ZRTOPnkkznzzDOb7Gvt2rVMnjyZWbNmceCBB3Luuefyox/9iCuuuAKAvn378sQTT/DDH/6QG264gZ/+9KdNtq+EbsGLKUHUA3PS4RHgyoj4RBHbZd03Udivx/XAnpLmkfQWO5ekeinZQfIMit8CV0TEqqw3iYhpEVEXEXX9+vUrIiwz64jyq5nyq5duv/12xowZw+jRo1mwYEGT6qBCf/vb3zjttNOorq5mt91245RTtjxMk/nz53PMMccwfPhwpk+fzoIFC1qM59lnn2XIkCEceOCBAJx33nnMnt14bc/pp58OwGGHHbalg798GzZs4KKLLmL48OGcddZZW+Iutlvw6sLeELdDMfdB3AGsjYhNkLQtSKqOiNa6FGygaa+vNcCr+SukJ/3z0/0KeCkdkNSNJDlMj4jfFRGnmVWI5n7pl9LHPvYxPv/5z/PEE0/w3nvvMWbMGF566SVuuOEGHn/8cfbcc08mT57cbDffOc3dEzx58mRmzpzJyJEjufXWW3nggQda3E9r/dzlugxvrkvx/G7BN2/evOVZEO3ZLXgxJYhZQM+86Z5AMU30jwMHSBoiqTtwDnBX/gqS9kiXAVwIzI6IVWmy+BnwTER8p4j3MrNOrnfv3kyYMIELLrhgS+lh1apV9OrVi913353XX3+de++9t8V9HHvssdx555289957rF69mt///vdblq1evZp9992XDRs2MD3v2ai77rorq1dv/aDNoUOHsnjxYhYtWgQkvbIed9xxRX+eSugWvJgE0SMi3slNpOOtll0iYiPwWeCPJI3Mt0fEAkkXS7o4Xe1gYIGkhSRXO+UuZx0PfBL4gKR56fCRoj+VmXVKEydO5Mknn+Scc84BYOTIkYwePZpDDjmECy64gPHjx7e4/ZgxYzj77LMZNWoUZ5xxBsccc8yWZd/85jc54ogj+OAHP8jQoUO3zD/nnHP493//d0aPHt2kYbhHjx7ccsstnHXWWQwfPpwuXbpw8cUXU6xK6Ba8mO6+HwIuy11VJOkw4PsRcdQOv3sbc3ffZuXj7r4r37Z2911MG8QVwG8k5doP9iV5BKmZmXVgxfTF9LikocBBJFcmLYyI7Kd9m5lZh9FqG4SkzwC9ImJ+RPwd6C2p+fvXzcysQyimkfqiiHg7NxERbwEXlSwiM9tplepyS9tx2/PdFJMguuQ/LCjtY6l7C+ubWSfUo0cPVqxY4SRRgSKCFStWbLmXoljFNFL/Ebhd0o9J7oS+GGj5YmIz63RqampoaGhg+fLl5Q7FMvTo0YOamppt2qaYBHElSV9Hl5A0Us8luZLJzGyLbt26MWTIkHKHYW2omO6+NwP/C7wI1AHHk9z4ZmZmHVizJQhJB5J0jzERWAH8GiAi3t8+oZmZWTm1VMW0EPgb8NGIWAQgaWq7RGVmZmXXUhXTGcAy4H5JP5F0PNldeJuZWQfUbIKIiDsj4mxgKPAAMBXoL+lHkj7UTvGZmVmZFNNI/W5ETI+Ik0me6TAPuKrUgZmZWXlt0zOpI+LNiPiviPhAqQIyM7PKsE0JwszMOg8nCDMzy+QEYWZmmZwgzMwskxOEmZllcoIwM7NMThBmZpbJCcLMzDI5QZiZWSYnCDMzy+QEYWZmmZwgzMwskxOEmZllcoIwM7NMThBmZpappAlC0omSnpW0SNJWDxmStKekOyU9JekxSYcWu62ZmZVWyRKEpCrgB8CHgWHAREnDCla7GpgXESOAc4HvbcO2ZmZWQqUsQYwFFkXEixGxHpgBnFqwzjBgFkBELARqJfUvclszMyuhUiaIAcDSvOmGdF6+J4HTASSNBQaTPPe6mG1Jt5siqV5S/fLly9sodDMzK2WCUMa8KJi+HthT0jzgMmAusLHIbZOZEdMioi4i6vr167cD4ZqZWb6uJdx3AzAwb7oGeDV/hYhYBZwPIEnAS+lQ3dq2ZmZWWqUsQTwOHCBpiKTuwDnAXfkrSNojXQZwITA7TRqtbmtmZqVVshJERGyU9Fngj0AVcHNELJB0cbr8x8DBwC8kbQKeBj7V0ralitXMzLamiMyq/Z1SXV1d1NfXlzsMM7OdhqQ5EVGXtcx3UpuZWSYnCDMzy+QEYWZmmZwgzMwskxOEmZllcoIwM7NMThBmZpbJCcLMzDI5QZiZWSYnCDMzy+QEYWZmmZwgzMwskxOEmZllcoIwM7NMThBmZpbJCcLMzDI5QZiZWSYnCDMzy+QEYWZmmZwgzMwskxOEmZllcoIwM7NMThBmZpbJCcLMzDI5QZiZWSYnCDMzy+QEYWZmmZwgKsiGDeWOwMysUddyB9BZrVsHTz0Fjz3WODz7LAwZAscdB8cem7zW1oJU7mjNrDNygmgHmzfD8883TQbz5sH69cny/v3hiCPg9NPhmWfgrrvglluSZQMHNiaL446DAw5wwjCz9lHSBCHpROB7QBXw04i4vmD57sBtwKA0lhsi4pZ02VTgQiCAvwPnR8TaUsbbVl59NUkCjz/e+LpyZbKsd284/HCYOjV5HTsWamqanvQ3b4ann4a//hVmz4b77oPp05Nl++zTNGEcfDB0cUWhmZWAIqI0O5aqgOeADwINwOPAxIh4Om+dq4HdI+JKSf2AZ4F9gH7Ag8CwiHhP0u3APRFxa0vvWVdXF/X19SX5PM1ZtQrq65uWDl55JVnWtSuMHJkkgdxw0EFQVbVt7xEBzz3XmDD++ldoaEiW9enTmDCOPRZGjNj2/ZtZ5yVpTkTUZS0rZQliLLAoIl5Mg5gBnAo8nbdOALtKEtAbeBPYmBdbT0kbgGrg1RLGWpT167duN1i4MDmBQ1L9M2FCYzIYNQp69Njx95WSxHLQQTBlSvJ+L73UmCz++le4885k3d13h2OOaUwYY8YkicrMbFuV8tQxAFiaN90AHFGwzveBu0hO/rsCZ0fEZuAVSTcAS4D3gD9FxJ+y3kTSFGAKwKBBg9os+M2bYdGipslg7tym7QZjx8LHP5681tXBXnu12du3SIL990+GyZOTeUuXNiaM2bPh7ruT+b17w/jxjaWMww+H7t3bJ04z27mVsorpLOCfI+LCdPqTwNiIuCxvnTOB8cDngfcBfwZGkrRZ/BY4G3gb+A1wR0Tc1tJ77kgV02uvNSaCxx9PhrffTpb17p0kgFybwdixSeNxJTcWL1vWNGHMn5/M79kTjjyysQ3jiCOSeZViwwZYsyYZ1q9PSj9du0K3bluPV/LxN9tZlKuKqQEYmDddw9bVROcD10eSpRZJegkYCgwGXoqI5QCSfgeMI2nQblPr1ydVQ0uWJNNduyb1+Oec05gMhg7d+er199kH/s//SQaAN96Av/2tMWl8/etJVVX37slnzCWMo45KEmKW/JP3u+82jmdNF7NO1jbbci9Ily6NySIrgeSP78jy3XZLfhAMGpS8DhzY/DEy60hKmSAeBw6QNAR4BTgH+HjBOkuA44G/SeoPHAS8CAg4UlI1SRXT8UBJWp+7d4ezzkr+6XPtBpX0i7qt9O0Lp52WDJCUjh56qLEN4/rr4brrkpPisGFJ8ig8kW/c2OJbZOrRA6qroVev5DU37LprUk1XOD9/3W7dYNOm5H03bGj6uiPjGzYk96EUu93q1Y3tTDl77NGYLPITR26oqYFddtnRb82svEpWxQQg6SPAjSRVRjdHxHWSLgaIiB9L2g+4FdiXJClcn6tGkvR1kiqmjcBc4MKIWNfS+5XjKqaO4p134OGHk2Tx5JNJ4iw8YTd3Im9uXs+eHeMS3A0bkkuXly5NSppLl249vPHG1tvtvXd28sgllX333flKptbxtFTFVNIE0d6cIKxc1qxJLj3OSh65pLJ6ddNtqqpgv/2yk0duvF8/t7VYaZWrDcKs06iuhgMPTIbmrFyZnTiWLoU5c2DmzKTqK98uuyTVVbnEMXhwMuTGBw7smFWiVhmcIMzaye67J8Ohh2Yvj0iqqrKqspYsgfvvT27C3Ly56XZ77900aeSPDxqUXH69s5VCNm6E5cvh9deTYdmypuMrVyYXkxx9dHJhxZ57ljvijslVTGY7kQ0bkiSxZAm8/HIy5MZzr++913SbXr1aTiD77dc+N1Nu2tR40i884ReOv/HG1hcGQFJS22ef5DM980zjhROHHpoki9wwaNDOlxR3xLvvJsdke7gNwqyTyJVCshJIbrywQb2qKqnGyk8a+Ylk0KDmTz75J/3WTvzNnfR79kxO+v37N742N55/efGaNcl9Sw8+mAyPPJJ0fQMwYEDThDF8eMe4IGDjxqTbnSefTIannmq8qOSll7Zvn04QZrbFu+8miaKw5JEbb2hITvz5+vZtbDxfs6bpL/3CKi9oetJv6YSfO+m3xa/9TZuSG0JzCePBBxv7LNt116QqKpcwxo7d/l/c7eXNN7dOBAsWNLZTdeuWdNY5cmRyef7Uqdt3HJ0gzKxoGzcml/VmJZCGhuTEWswv/Uqo4lmypGnCmD8/KcV07Zr0UzZ+fJIwxo9P4i6HjRuTxwHkkkAuIeSSGyTtTCNHJsOIEcnr0KFt022OE4SZGckNoo880pgwHnsM1qYPETjggMZkcfTRyRVpbZ3k3npr60Qwf35jDF27NpYK8pNBKZOXE4SZWYZ16+CJJ5JeBXJJY8WKZFnfvk0Txpgxxf9i37Qp6eyzsIpoaV73pf36bZ0IDj64/TvTdIIwMytCRPLo3wcfbEwaixYly3r0SDq3zCWNo45Kulx5++3GBJB7nT+/8Wqyrl2T6qD8RJArFVRCNZwThJnZdlq2rDFZPPRQUuLYtCk5uffvnyzP6dt360Rw8MGV3S+X76Q2M9tO++wDZ5yRDJBcBfboo0nCePHFxjaDESOS/rUqoVTQVpwgzMy2Qa9e8IEPJENH1wH62jQzs1JwgjAzs0xOEGZmlskJwszMMjlBmJlZJicIMzPL5ARhZmaZnCDMzCxTh+pqQ9Jy4OVyx7GD+gJvtLpW5+Bj0ZSPR1M+Ho125FgMjoh+WQs6VILoCCTVN9cvSmfjY9GUj0dTPh6NSnUsXMVkZmaZnCDMzCyTE0TlmVbuACqIj0VTPh5N+Xg0KsmxcBuEmZllcgnCzMwyOUGYmVkmJ4gKIGmgpPslPSNpgaTPlTumcpNUJWmupLvLHUu5SdpD0h2SFqZ/I0eVO6ZykjQ1/T+ZL+m/JfUod0ztSdLNkv4haX7evL0k/VnS8+nrnm3xXk4QlWEj8C8RcTBwJPAZScPKHFO5fQ54ptxBVIjvAX+IiKHASDrxcZE0ALgcqIuIQ4Eq4JzyRtXubgVOLJh3FTArIg4AZqXTO8wJogJExGsR8UQ6vprkBDCgvFGVj6Qa4CTgp+WOpdwk7QYcC/wMICLWR8TbZQ2q/LoCPSV1BaqBV8scT7uKiNnAmwWzTwV+no7/HPhYW7yXE0SFkVQLjAYeLXMo5XQj8EVgc5njqAT7A8uBW9Iqt59K6lXuoMolIl4BbgCWAK8BKyPiT+WNqiL0j4jXIPnBCezdFjt1gqggknoDvwWuiIhV5Y6nHCSdDPwjIuaUO5YK0RUYA/woIkYD79JG1Qc7o7Ru/VRgCLAf0EvSJ8obVcflBFEhJHUjSQ7TI+J35Y6njMYDp0haDMwAPiDptvKGVFYNQENE5EqUd5AkjM7qBOCliFgeERuA3wHjyhxTJXhd0r4A6es/2mKnThAVQJJI6pifiYjvlDuecoqIL0VETUTUkjQ+/iUiOu0vxIhYBiyVdFA663jg6TKGVG5LgCMlVaf/N8fTiRvt89wFnJeOnwf8v7bYade22IntsPHAJ4G/S5qXzrs6Iu4pX0hWQS4DpkvqDrwInF/meMomIh6VdAfwBMnVf3PpZF1uSPpvYALQV1IDcA1wPXC7pE+RJNGz2uS93NWGmZllcRWTmZllcoIwM7NMThBmZpbJCcLMzDI5QZiZWSYnCLNWSNokaV7e0GZ3Mkuqze+V06yS+D4Is9a9FxGjyh2EWXtzCcJsO0laLOnfJD2WDv+Uzh8saZakp9LXQen8/pLulPRkOuS6iKiS9JP0GQd/ktQzXf9ySU+n+5lRpo9pnZgThFnrehZUMZ2dt2xVRIwFvk/SCy3p+C8iYgQwHbgpnX8T8NeIGEnSn9KCdP4BwA8i4hDgbeCMdP5VwOh0PxeX5qOZNc93Upu1QtI7EdE7Y/5i4AMR8WLa2eKyiOgj6Q1g34jYkM5/LSL6SloO1ETEurx91AJ/Th/0gqQrgW4R8S1JfwDeAWYCMyPinRJ/VLMmXIIw2zHRzHhz62RZlze+ica2wZOAHwCHAXPSB+SYtRsnCLMdc3be6yPp+MM0PgZzEvBgOj4LuAS2PHN7t+Z2KqkLMDAi7id5eNIewFalGLNS8i8Ss9b1zOtlF5LnQ+cudd1F0qMkP7YmpvMuB26W9AWSp8Hlel/9HDAt7XFzE0myeK2Z96wCbpO0OyDgu37UqLU3t0GYbae0DaIuIt4odyxmpeAqJjMzy+QShJmZZXIJwszMMjlBmJlZJicIMzPL5ARhZmaZnCDMzCzT/welw2T5rP4eYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the training loss decreases with every eoch, and the training accuracy increases with every epoch. That's what you would expect when running gradient-descent optimization - the quantity you're trying to minimize should be less with every iteration. But that isn't the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example fo what we warned against earlier: a model that performs better on the training data isn't necessarily a model that will do better on data it has never seen before. In precise terms, what you're seeing is _overfitting_: after the second epoch, you're overoptimizing on the training data, and you end up learning representations that are specific to the training data and don't generalize to data outside of the training set.\n",
    "\n",
    "In this case, to prevent overfitting, you could stop training after three epochs. In general, you can use a range of techniques to mitigate overfitting, which we'll cover later. \n",
    "\n",
    "Let's train a new network from scratch for four epochs and then evalute it on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Retraining a model from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3153 - accuracy: 0.8712\n",
      "Epoch 2/4\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2179 - accuracy: 0.9163\n",
      "Epoch 3/4\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.1919 - accuracy: 0.9284\n",
      "Epoch 4/4\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.1762 - accuracy: 0.9348\n",
      "782/782 [==============================] - 1s 910us/step - loss: 0.3164 - accuracy: 0.8844\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=16)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31644323468208313, 0.8843600153923035]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fairly naive approach achieves an accuracy of 88%. With SOTA approaches, you should be able to get close to 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Using a trained model to generate predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08987561],\n",
       "       [0.99992573],\n",
       "       [0.98327047],\n",
       "       ...,\n",
       "       [0.16351113],\n",
       "       [0.05437252],\n",
       "       [0.42116553]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Further experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.3497 - accuracy: 0.8529 - val_loss: 0.2773 - val_accuracy: 0.8865\n",
      "Epoch 2/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.2190 - accuracy: 0.9173 - val_loss: 0.2857 - val_accuracy: 0.8887\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.1809 - accuracy: 0.9335 - val_loss: 0.3429 - val_accuracy: 0.8824\n",
      "Epoch 4/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.1593 - accuracy: 0.9443 - val_loss: 0.3512 - val_accuracy: 0.8861\n",
      "Epoch 5/20\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.1397 - accuracy: 0.9513 - val_loss: 0.3589 - val_accuracy: 0.8783\n",
      "Epoch 6/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.1209 - accuracy: 0.9597 - val_loss: 0.3995 - val_accuracy: 0.8806\n",
      "Epoch 7/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.1032 - accuracy: 0.9670 - val_loss: 0.5423 - val_accuracy: 0.8754\n",
      "Epoch 8/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.0988 - accuracy: 0.9687 - val_loss: 0.4448 - val_accuracy: 0.8621\n",
      "Epoch 9/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.0853 - accuracy: 0.9737 - val_loss: 0.4514 - val_accuracy: 0.8644\n",
      "Epoch 10/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.0794 - accuracy: 0.9749 - val_loss: 0.7903 - val_accuracy: 0.8715\n",
      "Epoch 11/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.0757 - accuracy: 0.9785 - val_loss: 0.6130 - val_accuracy: 0.8707\n",
      "Epoch 12/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.0687 - accuracy: 0.9809 - val_loss: 0.7855 - val_accuracy: 0.8692\n",
      "Epoch 13/20\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.0635 - accuracy: 0.9823 - val_loss: 0.9016 - val_accuracy: 0.8656\n",
      "Epoch 14/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.0646 - accuracy: 0.9819 - val_loss: 0.7799 - val_accuracy: 0.8550\n",
      "Epoch 15/20\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.0574 - accuracy: 0.9841 - val_loss: 0.8727 - val_accuracy: 0.8649\n",
      "Epoch 16/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.0514 - accuracy: 0.9856 - val_loss: 1.0888 - val_accuracy: 0.8607\n",
      "Epoch 17/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.0475 - accuracy: 0.9867 - val_loss: 1.0926 - val_accuracy: 0.8538\n",
      "Epoch 18/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.0516 - accuracy: 0.9861 - val_loss: 1.2185 - val_accuracy: 0.8580\n",
      "Epoch 19/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.0434 - accuracy: 0.9883 - val_loss: 1.1998 - val_accuracy: 0.8547\n",
      "Epoch 20/20\n",
      "1250/1250 [==============================] - 1s 1ms/step - loss: 0.0427 - accuracy: 0.9877 - val_loss: 1.5349 - val_accuracy: 0.8435\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(8, activation=\"relu\"),\n",
    "    layers.Dense(8, activation=\"relu\"),\n",
    "    layers.Dense(8, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.003),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=12,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 10000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.0311 - accuracy: 0.9909 - val_loss: 1.4963 - val_accuracy: 0.8485\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 0.0226 - accuracy: 0.9919 - val_loss: 1.4976 - val_accuracy: 0.8464\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.0202 - accuracy: 0.9926 - val_loss: 1.4979 - val_accuracy: 0.8441\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0189 - accuracy: 0.9930 - val_loss: 1.5322 - val_accuracy: 0.8421\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.0177 - accuracy: 0.9934 - val_loss: 1.5800 - val_accuracy: 0.8417\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.0169 - accuracy: 0.9937 - val_loss: 1.5536 - val_accuracy: 0.8407\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.0162 - accuracy: 0.9942 - val_loss: 1.6185 - val_accuracy: 0.8398\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.0155 - accuracy: 0.9943 - val_loss: 1.6084 - val_accuracy: 0.8398\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.0152 - accuracy: 0.9945 - val_loss: 1.6307 - val_accuracy: 0.8371\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0146 - accuracy: 0.9945 - val_loss: 1.6478 - val_accuracy: 0.8360\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0140 - accuracy: 0.9948 - val_loss: 1.6547 - val_accuracy: 0.8343\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.0138 - accuracy: 0.9951 - val_loss: 1.6600 - val_accuracy: 0.8315\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.0132 - accuracy: 0.9951 - val_loss: 1.7012 - val_accuracy: 0.8322\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0130 - accuracy: 0.9954 - val_loss: 1.6873 - val_accuracy: 0.8302\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.0127 - accuracy: 0.9955 - val_loss: 1.7266 - val_accuracy: 0.8288\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.0122 - accuracy: 0.9955 - val_loss: 1.7091 - val_accuracy: 0.8300\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0121 - accuracy: 0.9957 - val_loss: 1.7899 - val_accuracy: 0.8276\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.0118 - accuracy: 0.9958 - val_loss: 1.7847 - val_accuracy: 0.8270\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.0116 - accuracy: 0.9957 - val_loss: 1.8194 - val_accuracy: 0.8269\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.0115 - accuracy: 0.9959 - val_loss: 1.8371 - val_accuracy: 0.8262\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=1512,\n",
    "                    validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Wrapping up\n",
    "\n",
    "\n",
    "Here's what you should take away from this example:\n",
    "\n",
    "* You usually need to do quite a bit of preprocessing on your raw data in order to be able to feed it - as tensors - into a neural network. Sequences of words can be encoded as binary vectors, but there are other encoding options too.\n",
    "* Stacks of Dense layers with **relu** activations can solve a wide range of problems (including sentiment classification), and you'll likely use them frequently.\n",
    "* In a binary classification problem (two output classes), your network should end with a Dense layer with one unit and a sigmoid activation: the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
    "* With such a scalar sigmoid output on a binary classifcation problem, the loss function you should use is **binary_crossentropy**.\n",
    "* The **rmsprop** optimizer is generally a good enough choice, whatever your problem. That's one less thing for you to worry about.\n",
    "* As they get better on their training data, neural networks eventually start overfitting and end up obtaining increasingly worse results on data they've never seen before. Be sure to always monitor performance on data that is outside of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Classifying newswires: a multiclass classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The Reuters dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loading the Reuters dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
    "    num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Decoding newswires back to text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in\n",
    "    train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Encoding the input data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Encoding the labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "y_train = to_one_hot(train_labels)\n",
    "y_test = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(train_labels)\n",
    "y_test = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Building your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(46, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Compiling the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Validating your approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Setting aside a validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = y_train[:1000]\n",
    "partial_y_train = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting the training and validation loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting the training and validation accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Retraining a model from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(46, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "hits_array = np.array(test_labels) == np.array(test_labels_copy)\n",
    "hits_array.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Generating predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### A different way to handle the labels and the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The importance of having sufficiently large intermediate layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**A model with an information bottleneck**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(4, activation=\"relu\"),\n",
    "    layers.Dense(46, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Further experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Predicting house prices: a regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The Boston Housing Price dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loading the Boston housing dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Normalizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Building your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Validating your approach using K-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**K-fold validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print(f\"Processing fold #{i}\")\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=num_epochs, batch_size=16, verbose=0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Saving the validation logs at each fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print(f\"Processing fold #{i}\")\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=16, verbose=0)\n",
    "    mae_history = history.history[\"val_mae\"]\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Building the history of successive mean K-fold validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting validation scores, excluding the first 10 data points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "truncated_mae_history = average_mae_history[10:]\n",
    "plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training the final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.fit(train_data, train_targets,\n",
    "          epochs=130, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "test_mae_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Generating predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Chapter summary"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter04_getting-started-with-neural-networks.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
